help()
version
install.packages("swirl")
packageVersion("swirl")
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
library(tidyr)
students
?gather
quit
;
library("swirl"")
;
/
''
?
""
install.packages("swirl")
library(swirl)
install_from_swirl("Getting and Cleaning DAta")
swirl()
0
swirl()
install_from_swirl("Getting and Cleaning Data")
swirl()
path2csv
mydf <- read.csv(path2csv,stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
?tbl_df
cran
?select
select(cran,ip_id,package,country)
5:20
select(cran,r_arch:country)
select(cran,country:r_arch)
cran
select(cran,-time)
-5:20
-(5:20)
select(cran,-(X:size))
filter(cran, package=="swirl")
filter(cran,r_version=="3.1.1",country=="US")
?Comparison
filter(cran,r_version<=3.0.2,country=="IN")
filter(cran,r_version<="3.0.2",country=="IN")
filter(cran,country=="US"|country=="IN")
filter(cran,size>100500,r_os=="linux-gnu")
is.na(c(3,5,NA,10))
!is.na(c(3,5,NA,10))
filter(cran,r_version!="NA")
filter(cran,!is.na(r_version))
cran2 <- select(cran,size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran,country,desc(r_version),ip_id)
arrange(cran2,country,desc(r_version),ip_id)
cran3 <- select(ip_id,package,size)
cran3 <- select(cran,ip_id,package,size)
cran3
mutate(cran3, size_mb = size/2^20)
mutate(cran3,size_gb = size_mb(2^20))
mutate(cran3,size_mb = size/2^20,size_gb = size_mb/2^20)
mutate(cran3,size_mb = size/2^20,size_gb = size_mb/2^10)
mutate(cran3,correct_size=size-1000)
mutate(cran3,correct_size=size+1000)
summarize(cran,avg_bytes=mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by
by_package <- group_by(cran,package)
by_package
summarize(by_package)
summarize(by_package,mean(size))
submit()
pack_sum
quantile(pack_sum$count,probs=0.99)
filter(pack_sum,count>679)
top_counts <- filter(pack_sum,count>679)
top_counts
View(top_counts)
top_counts_sorted <- arrange(top_counts,count)
top_counts_sorted <- arrange(top_counts,desc(count))
View(top_counts_sorted)
quantile((pack_sum,unique,probs=0.99))
quantile(pack_sum,unique,probs=0.99)
quantile(pack_sum$unique, probs=0.99)
filter(pack_sum, unique>465)
top_unique <- filter(pack_sum, unique>465)
View(top_unique)
top_unique_sorted <- arrange(top_unique,desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students,sex,count,-grade)
students2
gather(students2,sex_class,count,-grade)
res <- gather(students2,sex_class,count,-grade)
res
?separate
separate(data=res,col=sex_class,into=c("sex","class"))
submit()
students3
?gather
submit()
?spread
submit()
submit()
submit()
submit()
submit()
submit()
library(readr)
parse_number("class5")
submit()
submit()
submit()
?mutate
students4
?select
submit()
submit()
?unique
history
submit()
passed
failed
?gather
passed <- mutate(passed, status="passed")
failed <- mutuate(failed,status="failed")
failed <- mutate(failed, status="failed")
?bind_rows
packageVersion("dplyr")
bind_rows(passed,failed)
sat
?separate
submit()
submit()
submit()
submit()
swirl()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package=lubridate)
today()
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day, label=TRUE)
this_moment <- now()
this_moment
hour(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
dmy("25081985")
dmy(25081985)
ymd("192012")
ymd("19/20/12")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours=8, minutes=34, seconds=55)
this_moment
this_moment <- now()
update(this_moment, hours=10,minutes=16,seconds=0)
this_moment
nyc <- now()
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart,hours=17,minutes=34)
depart
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive,tzone="Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008")
last_time <- mdy("June 17, 2008",tz="Singapore")
last_time
?interval
how_long <- interval(last_time, arrive)
as.period(how_long)
stopwatch()
ls
library(data.table)
install.packages("data.table")
library(data.table)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
fileUrl
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile = "america_comm_survey.csv")
data <- read.csv("america_comm_survey.csv")
summary(data)
head(data$VAL)
head(data)
head(data$VAL)
DT <- data.table(data)
DT
DT[,.N]
summary(DT)
DT
DT[,.N,by=VAL==24]
DT[,.N,VAL==24]
head(DT$FES)
install.packages("xlsx")
library("xslx")
library(xslx)
library(xlsx)
library("xlsx")
library(xlsx)
install.packages("xlsx")
library(xlsx)
filUrl <- “https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx”
filUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileXLS,destfile="NGAP.xlsx", mode = "wb")
download.file(fileUrl,destfile="NGAP.xlsx", mode = "wb")
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx("NGAP.xlsx",sheetIndex=1, colIndex = colIndex, rowIndex = rowIndex)
install.packages("RMySQL")
con <- url("https://scholar.google.com/citations?user=HI-I6COAAAAJ&hl=en")
htmlCode <- readLines(con)
con <- url("https://www.tandfonline.com/doi/abs/10.1080/00797308.1957.11822802?journalCode=upsc20/")
htmlCode <- readLines(con)
close(con)
htmlCode
library(XML)
library("XML")
install.packages("XML")
library(XML)
url <- "https://www.tandfonline.com/doi/abs/10.1080/00797308.1957.11822802?journalCode=upsc20"
html <- htmlTreeParse(url,useInternalNodes = T)
html <- htmlTreeParse(url,useInternalNodes=T)
html
xpathSApply(html,"//title",xmlValue)
url <- "https://scholar.google.com/citations?user=I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes=T)
xpathSApply(html,"//title",xmlValue)
url <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
xpathSApply(html,"//title",xmlValue)
xpathSApply(html,"//title",xmlValue)
html <- htmlTreeParse(url,useInternalNodes=T)
xpathSApply(html,"//title",xmlValue)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes=T)
xpathSApply(html,"//title",xmlValue)
html <- htmlTreeParse(url,useInternalNodes=T)
url <- https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en
url <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes=T)
html
html
html <- htmlTreeParse(url,useInternalNodes=T)
xpathSApply(html,"//title",xmlValue)
url <- "http://www.google.com"
html <- htmlTreeParse(url,useInternalNodes=T)
html
xpathSApply(html,"//title",xmlValue)
xpathSApply(html,"//span",xmlValue)
set.seed(12345)
x <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
x
x <- x[sample(1:5),]
x
x$var2[c(1,3)]=NA
x
x[x$var1<=3]
x[(x$var1<=3),]
x[(x$var1<=3 & x$var3<12),]
x[(x$var1<=3 & x$var2<11),]
x$var4 <- rnorm(5)
x
y <- cbind(x,rnorm(5))
y
y <- rbind(x,rnorm(5))
y
y <- rbind(rnorm(5),x)
y
if(!file.exists("./data")){dir.create("./data")}
library(reshape2)
install.packages("reshape2")
install.packages("reshape2")
install.packages("reshape2")
library(reshape2)
head(InsectSprays)
ddply(InsectSprays,.(spray),summarize,sum=sum(count))
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
ddply(InsectSprays,.(spray),summarize,sum=sum(count))
chicago <- readRDS("chicago.rds")
library(dplyr)
d1 = date()
d1
d2 = Sys.Date()
d2
class(d2)
format(d2,"%a %b %c")
format(d2,"%a %b %d")
weekdays(d2)
months(d2)
julian(d2)
swirl()
install.packages("swirl")
install.packages("swirl")
library(swirl)
swirl::install_course("Getting and Cleaning Data")
swirl()
Sys.getlocale("LC_ALL")
Sys.getlocale("LC_TIME")
library(lubridate)
help(package="lubridate")
help(package = "lubridate")
help(package = lubridate)
this_day = today()
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day,label=TRUE)
this_moment <- now()
this_moment
hour(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
dmy(25081975)
dmy(25081985)
ymd("192012")
ymd("19/20/12")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
now()
update(this_moment, hours = 15, minutes = 14, seconds = 06)
update(this_moment, hours = 15, minutes = 14, seconds = 6)
this_moment <- update(this_moment, hours = 15, minutes = 14, seconds = 6)
this_moment
now()
nyc <- now()
nyc <- now("America/New_York")
nyc
depart <- nyc+days(2)
depart
depart <- update(depart,hours=17,minutes=34)
depart
arrive <- depart + hours(15) + minutes(50)
?with_tz
with_tz(arrive,"Asia/Hong_Kong")
arrive <- with_tz(arrive,"Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008",tz="Singapore")
last_time
interval(last_time,arrive)
interval(arrive,last_time)
interval(arrive-last_time)
?interval
inveral(last_time,arrive,tz="Singapore")
interval(last_time,arrive,tz="Singapore")
interval(last_time,arrive,tz="Singapore")
how_long <- interval(last_time,arrive,tz="Singapore")
how_long <- interval(last_time,arrive)
as.period(how_long)
stopwatch()
install.packages("quantmod")
packages <- c("data.table", "quantmod")
sapply(packages, require, character.only = TRUE, quietly = TRUE)
setInternet2(TRUE)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
f <- file.path(getwd(),"ss06hid.csv")
download.file(url,f)
dt <- data.table(read.csv(f))
varname <- names(dt)
varname
strsplit(varname,"wgtp")
varname_split <- strsplit(varname,"wgtp")
varname_split[[123]]
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215, stringsAsFactors = FALSE))
names(dtGDP)
summary(dtGDP)
dtGDP
dtGDP <- dtGDP[X != ""]
dtGDP
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
dtGDP
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
dtGDP
gdp <- as.numeric(gsub(",", "", dtGDP$gdp))
mean(gdp)
mean(gdp,na.rm = TRUE)
iunited <- grepl("^United",dtGDP$Long.Name)
summary(iunited)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dtEd
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
dtGDP
dtEd
dtED <- dtED[ X != ""]
dtEd <- dtEd[ X != ""]
dtEd
dtEd <- dtEd[, list(X, X.1, X.3, X.4)]
dtEd
download.file(url, f)
dtEd <- data.table(read.csv(f))
dtEd
dtEd <- dtEd[ X != ""]
dtEd
dtEd <- dtEd[, list(X, Gross.domestic.product.2012, X.1, X.3, X.4)]
dtEd <- dtEd[, list(X, Gross.domestic.product.2012, X.2,X.3)]
download.file(url, f)
dtEd <- data.table(read.csv(f))
dtEd <- dtEd[ X != ""]
dtEd <- dtEd[, list(X, Gross.domestic.product.2012,X.2,X.3)]
dtEd
dtGDP
setnames(dtEd, c("X", "X.2", "X.3"), c("CountryCode","Long.Name","gdp")
)
dtEd
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
dt
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
zip_file <- "UCI HAR Dataset.zip"
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip", zip_file, mode = "wb")
unzip(zip_file)
data_dir <- "UCI HAR Dataset"
subject_test <- read.table(file.path(data_dir,"train","subject_test.txt"))
x_test <- read.table(file.path(data_dir,"train","X_test.txt"))
y_test <- read.table(file.path(data_dir,"train","Y_test.txt"))
getwd()
subject_test <- read.table(file.path(getwd(),data_dir,"train","subject_test.txt"))
subject_test <- read.table(file.path(getwd(),data_dir,"test","subject_test.txt"))
x_test <- read.table(file.path(getwd(),data_dir,"test","X_test.txt"))
y_test <- read.table(file.path(getwd(),data_dir,"test","Y_test.txt"))
subject_train <- read.table(file.path(getwd(),data_dir,"train","subject_train.txt"))
x_train <- read.table(file.path(getwd(),data_dir,"train","X_train.txt"))
y_train <- read.table(file.path(getwd(),data_dir,"train","Y_train.txt"))
features <- read.table(file.path(getwd(),data_dir,"features.txt"))
activities <- read.table(file.path(getwd(),data_dir,"activities.txt"))
activities <- read.table(file.path(getwd(),data_dir,"activity_labels.txt"))
subjects <- cbind(testSubjects,trainingSubjects)
setwd("/Users/t93ku6h/Documents/Development/coursera/getting-and-cleaning-data-course-project")
source("run_analysis.R")
